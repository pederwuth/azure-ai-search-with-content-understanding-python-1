{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Document Search with Azure Content Understanding\n",
    "## Objective\n",
    "This document illustrates an example workflow for how to leverage the Azure AI Content Understanding API to enhance the quality of document search.\n",
    "\n",
    "The sample will demonstrate the following steps:\n",
    "1. Extract the layout and content of a document using Azure AI Document Intelligence.\n",
    "2. For each figure in the document, extract its content with a custom analyzer using Azure AI Content Understanding, and insert it into the corresponding location in the document content.\n",
    "2. Chunk and embed the document content with LangChain and Azure OpenAI, and index them with Azure Search to generate an Azure Search index.\n",
    "3. Utilize an OpenAI chat model to search through content in the document with a natural language query.\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "1. Follow the [README](../README.md#configure-azure-ai-service-resource) to create the required resources for this sample.\n",
    "1. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\") or \"2024-12-01-preview\"\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\") or \"2024-11-30\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\") or \"2024-08-01-preview\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\") or \"2023-05-15\"\n",
    "\n",
    "# Load and validate Azure Search Services configs\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-index-visual-doc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the path to the file that will be analyzed\n",
    "# Sample report source: https://www.imf.org/en/Publications/CR/Issues/2024/07/18/United-States-2024-Article-IV-Consultation-Press-Release-Staff-Report-and-Statement-by-the-552100\n",
    "file = Path(\"../data/sample_report.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom analyzer using chart and diagram understanding template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer details for content-understanding-search-sample-39456ae0-d812-46e9-ac53-bf73a387d3cb:\n",
      "{\n",
      "  \"id\": \"29d52b86-6ad4-45c9-aae3-aa51de67b9a3\",\n",
      "  \"status\": \"Succeeded\",\n",
      "  \"result\": {\n",
      "    \"analyzerId\": \"content-understanding-search-sample-39456ae0-d812-46e9-ac53-bf73a387d3cb\",\n",
      "    \"description\": \"Extract detailed structured information from charts and diagrams.\",\n",
      "    \"createdAt\": \"2025-10-04T10:32:31Z\",\n",
      "    \"lastModifiedAt\": \"2025-10-04T10:32:31Z\",\n",
      "    \"config\": {\n",
      "      \"returnDetails\": false,\n",
      "      \"disableContentFiltering\": false\n",
      "    },\n",
      "    \"fieldSchema\": {\n",
      "      \"name\": \"ChartsAndDiagrams\",\n",
      "      \"fields\": {\n",
      "        \"Title\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Verbatim title of the chart.\"\n",
      "        },\n",
      "        \"ChartType\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"The type of chart.\",\n",
      "          \"enum\": [\n",
      "            \"area\",\n",
      "            \"bar\",\n",
      "            \"box\",\n",
      "            \"bubble\",\n",
      "            \"candlestick\",\n",
      "            \"funnel\",\n",
      "            \"heatmap\",\n",
      "            \"histogram\",\n",
      "            \"line\",\n",
      "            \"pie\",\n",
      "            \"radar\",\n",
      "            \"rings\",\n",
      "            \"rose\",\n",
      "            \"treemap\"\n",
      "          ],\n",
      "          \"enumDescriptions\": {\n",
      "            \"histogram\": \"Continuous values on the x-axis, which distinguishes it from bar.\",\n",
      "            \"rose\": \"In contrast to pie charts, the sectors are of equal angles and differ in how far each sector extends from the center of the circle.\"\n",
      "          }\n",
      "        },\n",
      "        \"TopicKeywords\": {\n",
      "          \"type\": \"array\",\n",
      "          \"description\": \"Relevant topics associated with the chart, used for tagging.\",\n",
      "          \"items\": {\n",
      "            \"type\": \"string\",\n",
      "            \"examples\": [\n",
      "              \"Business and finance\",\n",
      "              \"Arts and culture\",\n",
      "              \"Education and academics\"\n",
      "            ]\n",
      "          }\n",
      "        },\n",
      "        \"DetailedDescription\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Detailed description of the chart or diagram, not leaving out any key information. Include numbers, trends, and other details.\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Detailed summary of the chart, including highlights and takeaways.\"\n",
      "        },\n",
      "        \"MarkdownDataTable\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Underlying data of the chart in tabular markdown format. Give markdown output with valid syntax and accurate numbers, and fill any uncertain values with empty cells. If not applicable, output an empty string.\"\n",
      "        },\n",
      "        \"AxisTitles\": {\n",
      "          \"type\": \"object\",\n",
      "          \"description\": \"Titles of the x and y axes.\",\n",
      "          \"properties\": {\n",
      "            \"xAxisTitle\": {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            \"yAxisTitle\": {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"FootnotesAndAnnotations\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"All footnotes and textual annotations in the chart or diagram.\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"warnings\": [],\n",
      "    \"status\": \"ready\",\n",
      "    \"scenario\": \"image\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Get path to sample template\n",
    "ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/image_chart_diagram_understanding.json\"\n",
    "\n",
    "# Create analyzer\n",
    "ANALYZER_ID = \"content-understanding-search-sample-\" + str(uuid.uuid4())\n",
    "content_understanding_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/search_with_visusal_document\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = content_understanding_client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    print(f'Analyzer details for {result[\"result\"][\"analyzerId\"]}:')\n",
    "    print(json.dumps(result, indent=2))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Error in creating analyzer. Please double-check your analysis settings.\\nIf there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document analyzer for chapter extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document analyzer for chapter extraction...\n",
      "✓ Document analyzer created: document-chapter-analyzer-73d05d4d-0d9e-41d0-b8e6-b9895a3c01cb\n",
      "{\n",
      "  \"id\": \"bfc0c616-05ab-4e79-9521-4144f3c5b87c\",\n",
      "  \"status\": \"Succeeded\",\n",
      "  \"result\": {\n",
      "    \"analyzerId\": \"document-chapter-analyzer-73d05d4d-0d9e-41d0-b8e6-b9895a3c01cb\",\n",
      "    \"description\": \"Extract comprehensive chapter information from documents including hierarchy, summaries, and metadata.\",\n",
      "    \"createdAt\": \"2025-10-04T10:34:12Z\",\n",
      "    \"lastModifiedAt\": \"2025-10-04T10:34:13Z\",\n",
      "    \"config\": {\n",
      "      \"returnDetails\": false,\n",
      "      \"enableOcr\": true,\n",
      "      \"enableLayout\": true,\n",
      "      \"enableBarcode\": false,\n",
      "      \"enableFormula\": false,\n",
      "      \"disableContentFiltering\": false\n",
      "    },\n",
      "    \"fieldSchema\": {\n",
      "      \"name\": \"DocumentWithChapters\",\n",
      "      \"description\": \"Document structure with chapters, sections, and metadata.\",\n",
      "      \"fields\": {\n",
      "        \"document_title\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"The title of the entire document\"\n",
      "        },\n",
      "        \"document_summary\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"A brief summary of the entire document's content and purpose\"\n",
      "        },\n",
      "        \"chapters\": {\n",
      "          \"type\": \"array\",\n",
      "          \"description\": \"List of all chapters, sections, and major headings in the document\",\n",
      "          \"items\": {\n",
      "            \"type\": \"object\",\n",
      "            \"description\": \"Information about a single chapter or section\",\n",
      "            \"properties\": {\n",
      "              \"title\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"The exact chapter or section title\"\n",
      "              },\n",
      "              \"number\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Chapter number or identifier (e.g., '1', 'Chapter 1', 'I', 'Section A')\"\n",
      "              },\n",
      "              \"level\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Hierarchy level - '1' for main chapters, '2' for subsections, '3' for sub-subsections, etc.\"\n",
      "              },\n",
      "              \"summary\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"A concise 2-3 sentence summary of this chapter's content and key points\"\n",
      "              },\n",
      "              \"key_topics\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Comma-separated list of 3-5 main topics, themes, or concepts covered in this chapter\"\n",
      "              },\n",
      "              \"page_range\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Page range for this chapter in format 'start-end' (e.g., '5-12')\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"warnings\": [],\n",
      "    \"status\": \"ready\",\n",
      "    \"scenario\": \"document\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get path to chapter extraction template\n",
    "DOCUMENT_ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/document_with_chapters.json\"\n",
    "\n",
    "# Create document analyzer for chapter extraction\n",
    "DOCUMENT_ANALYZER_ID = \"document-chapter-analyzer-\" + str(uuid.uuid4())\n",
    "\n",
    "try:\n",
    "    print(\"Creating document analyzer for chapter extraction...\")\n",
    "    response = content_understanding_client.begin_create_analyzer(\n",
    "        DOCUMENT_ANALYZER_ID, \n",
    "        analyzer_template_path=DOCUMENT_ANALYZER_TEMPLATE_PATH\n",
    "    )\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    print(f'✓ Document analyzer created: {DOCUMENT_ANALYZER_ID}')\n",
    "    print(json.dumps(result, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error details: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    if hasattr(e, 'response'):\n",
    "        print(f\"Response status: {e.response.status_code}\")\n",
    "        print(f\"Response body: {e.response.text}\")\n",
    "    print(\"\\nError in creating document analyzer. Please check your template configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze document layout and compose with figure descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "# Define helper functions for document-figure composition\n",
    "def insert_figure_contents(md_content, figure_contents, span_offsets):\n",
    "    \"\"\"\n",
    "    Inserts the figure content for each of the provided figures in figure_contents\n",
    "    before the span offset of that figure in the given markdown content.\n",
    "\n",
    "    Args:\n",
    "    - md_content (str): The original markdown content.\n",
    "    - figure_contents (list[str]): The contents of each figure to insert.\n",
    "    - span_offsets (list[int]): The span offsets of each figure in order. These should be sorted and strictly increasing.\n",
    "\n",
    "    Returns:\n",
    "    - str: The modified markdown content with the the figure contents prepended to each figure's span.\n",
    "    \"\"\"\n",
    "    # NOTE: In this notebook, we only alter the Markdown content returned by the Document Intelligence API,\n",
    "    # and not the per-element spans in the API response. Thus, after figure content insertion, these per-element spans will be inaccurate.\n",
    "    # This may impact use cases like citation page number calculation.\n",
    "    # Additional code may be needed to correct the spans or otherwise infer the page numbers for each citation.\n",
    "    # The main purpose of the notebook is to show the feasibility of using Content Understanding with Azure Search for RAG chat applications.\n",
    "\n",
    "    # Validate span_offsets are sorted and strictly increasing\n",
    "    if span_offsets != sorted(span_offsets) or not all([o < span_offsets[i + 1] for i, o in enumerate(span_offsets) if i < len(span_offsets) - 1]):\n",
    "        raise ValueError(\"span_offsets should be sorted and strictly increasing.\")\n",
    "\n",
    "    # Split the content based on the provided spans\n",
    "    parts = []\n",
    "    preamble = None\n",
    "    for i, offset in enumerate(span_offsets):\n",
    "        if i == 0 and offset > 0:\n",
    "            preamble = md_content[0:offset]\n",
    "            parts.append(md_content[offset:span_offsets[i + 1]])\n",
    "        elif i == len(span_offsets) - 1:\n",
    "            parts.append(md_content[offset:])\n",
    "        else:\n",
    "            parts.append(md_content[offset:span_offsets[i + 1]])\n",
    "\n",
    "    # Join the parts back together with the figure content inserted\n",
    "    modified_content = \"\"\n",
    "    if preamble:\n",
    "        modified_content += preamble\n",
    "    for i, part in enumerate(parts):\n",
    "        modified_content += f\"<!-- FigureContent=\\\"{figure_contents[i]}\\\" -->\" + part\n",
    "\n",
    "    return modified_content\n",
    "\n",
    "def crop_image_from_pdf_page(pdf_path, page_number, bounding_box):\n",
    "    \"\"\"\n",
    "    Crops a region from a given page in a PDF and returns it as an image.\n",
    "\n",
    "    Args:    \n",
    "    - pdf_path (pathlib.Path): Path to the PDF file.\n",
    "    - page_number (int): The page number to crop from (0-indexed).\n",
    "    - bounding_box (tuple): A tuple of (x0, y0, x1, y1) coordinates for the bounding box.\n",
    "    \n",
    "    Returns:\n",
    "    - PIL.Image: A PIL Image of the cropped area.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(page_number)\n",
    "    \n",
    "    # Cropping the page. The rect requires the coordinates in the format (x0, y0, x1, y1).\n",
    "    bbx = [x * 72 for x in bounding_box]\n",
    "    rect = fitz.Rect(bbx)\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(300 / 72, 300 / 72), clip=rect)\n",
    "    \n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "    doc.close()\n",
    "\n",
    "    return img\n",
    "\n",
    "def format_content_understanding_result(content_understanding_result):\n",
    "    \"\"\"\n",
    "    Formats the JSON output of the Content Understanding result as Markdown for downstream usage in text.\n",
    "    \n",
    "    Args:\n",
    "    - content_understanding_result (dict): A dictionary containing the output from Content Understanding.\n",
    "\n",
    "    Returns:\n",
    "    - str: A Markdown string of the result content.\n",
    "    \"\"\"\n",
    "    def _format_result(key, result):\n",
    "        result_type = result[\"type\"]\n",
    "        if result_type in [\"string\", \"integer\", \"number\", \"boolean\"]:\n",
    "            return f\"**{key}**: \" + str(result[f'value{result_type.capitalize()}']) + \"\\n\"\n",
    "        elif result_type == \"array\":\n",
    "            return f\"**{key}**: \" + ', '.join([str(result[\"valueArray\"][i][f\"value{r['type'].capitalize()}\"]) for i, r in enumerate(result[\"valueArray\"])]) + \"\\n\"\n",
    "        elif result_type == \"object\":\n",
    "            return f\"**{key}**\\n\" + ''.join([_format_result(f\"{key}.{k}\", result[\"valueObject\"][k]) for k in result[\"valueObject\"]])\n",
    "\n",
    "    fields = content_understanding_result['result']['contents'][0]['fields']\n",
    "    markdown_result = \"\"\n",
    "    for field in fields:\n",
    "        markdown_result += _format_result(field, fields[field])\n",
    "\n",
    "    return markdown_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: Extract chapters with Content Understanding\n",
      "======================================================================\n",
      "✓ Extracted 4 chapters\n",
      "✓ Document: 2024 ARTICLE IV CONSULTATION-PRESS RELEASE; STAFF REPORT; AND STATEMENT BY THE EXECUTIVE DIRECTOR FOR THE UNITED STATES\n",
      "  1. UNITED STATES (Pages 0-0, Level 1)\n",
      "  2. 2024 ARTICLE IV CONSULTATION-PRESS RELEASE; STAFF REPORT; AND STATEMENT BY THE EXECUTIVE DIRECTOR FOR THE UNITED STATES (Pages 0-0, Level 2)\n",
      "  3. IMF Executive Board Concludes 2024 Article IV Consultation with the United States (Pages 0-0, Level 1)\n",
      "  4.  (Pages 0-0, Level 3)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Extract layout and figures with Document Intelligence\n",
      "======================================================================\n",
      "✓ Extracted 4 chapters\n",
      "✓ Document: 2024 ARTICLE IV CONSULTATION-PRESS RELEASE; STAFF REPORT; AND STATEMENT BY THE EXECUTIVE DIRECTOR FOR THE UNITED STATES\n",
      "  1. UNITED STATES (Pages 0-0, Level 1)\n",
      "  2. 2024 ARTICLE IV CONSULTATION-PRESS RELEASE; STAFF REPORT; AND STATEMENT BY THE EXECUTIVE DIRECTOR FOR THE UNITED STATES (Pages 0-0, Level 2)\n",
      "  3. IMF Executive Board Concludes 2024 Article IV Consultation with the United States (Pages 0-0, Level 1)\n",
      "  4.  (Pages 0-0, Level 3)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Extract layout and figures with Document Intelligence\n",
      "======================================================================\n",
      "✓ Extracted document content (280293 characters)\n",
      "✓ Detected 60 figures\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Analyze figures with Content Understanding\n",
      "======================================================================\n",
      "Extracting figure contents with Content Understanding image analyzer...\n",
      "✓ Extracted document content (280293 characters)\n",
      "✓ Detected 60 figures\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Analyze figures with Content Understanding\n",
      "======================================================================\n",
      "Extracting figure contents with Content Understanding image analyzer...\n",
      "  ✓ Figure 1 analyzed (page 1)\n",
      "  ✓ Figure 1 analyzed (page 1)\n",
      "  ✓ Figure 2 analyzed (page 2)\n",
      "  ✓ Figure 2 analyzed (page 2)\n",
      "  ✓ Figure 3 analyzed (page 5)\n",
      "  ✓ Figure 3 analyzed (page 5)\n",
      "  ✓ Figure 4 analyzed (page 9)\n",
      "  ✓ Figure 4 analyzed (page 9)\n",
      "  ✓ Figure 5 analyzed (page 10)\n",
      "  ✓ Figure 5 analyzed (page 10)\n",
      "  ✓ Figure 6 analyzed (page 10)\n",
      "  ✓ Figure 6 analyzed (page 10)\n",
      "  ✓ Figure 7 analyzed (page 10)\n",
      "  ✓ Figure 7 analyzed (page 10)\n",
      "  ✓ Figure 8 analyzed (page 11)\n",
      "  ✓ Figure 8 analyzed (page 11)\n",
      "  ✓ Figure 9 analyzed (page 12)\n",
      "  ✓ Figure 9 analyzed (page 12)\n",
      "  ✓ Figure 10 analyzed (page 12)\n",
      "  ✓ Figure 10 analyzed (page 12)\n",
      "  ✓ Figure 11 analyzed (page 12)\n",
      "  ✓ Figure 11 analyzed (page 12)\n",
      "  ✓ Figure 12 analyzed (page 12)\n",
      "  ✓ Figure 12 analyzed (page 12)\n",
      "  ✓ Figure 13 analyzed (page 13)\n",
      "  ✓ Figure 13 analyzed (page 13)\n",
      "  ✓ Figure 14 analyzed (page 13)\n",
      "  ✓ Figure 14 analyzed (page 13)\n",
      "  ✓ Figure 15 analyzed (page 13)\n",
      "  ✓ Figure 15 analyzed (page 13)\n",
      "  ✓ Figure 16 analyzed (page 14)\n",
      "  ✓ Figure 16 analyzed (page 14)\n",
      "  ✓ Figure 17 analyzed (page 14)\n",
      "  ✓ Figure 17 analyzed (page 14)\n",
      "  ✓ Figure 18 analyzed (page 14)\n",
      "  ✓ Figure 18 analyzed (page 14)\n",
      "  ✓ Figure 19 analyzed (page 14)\n",
      "  ✓ Figure 19 analyzed (page 14)\n",
      "  ✓ Figure 20 analyzed (page 15)\n",
      "  ✓ Figure 20 analyzed (page 15)\n",
      "  ✓ Figure 21 analyzed (page 16)\n",
      "  ✓ Figure 21 analyzed (page 16)\n",
      "  ✓ Figure 22 analyzed (page 16)\n",
      "  ✓ Figure 22 analyzed (page 16)\n",
      "  ✓ Figure 23 analyzed (page 17)\n",
      "  ✓ Figure 23 analyzed (page 17)\n",
      "  ✓ Figure 24 analyzed (page 17)\n",
      "  ✓ Figure 24 analyzed (page 17)\n",
      "  ✓ Figure 25 analyzed (page 17)\n",
      "  ✓ Figure 25 analyzed (page 17)\n",
      "  ✓ Figure 26 analyzed (page 18)\n",
      "  ✓ Figure 26 analyzed (page 18)\n",
      "  ✓ Figure 27 analyzed (page 18)\n",
      "  ✓ Figure 27 analyzed (page 18)\n",
      "  ✓ Figure 28 analyzed (page 18)\n",
      "  ✓ Figure 28 analyzed (page 18)\n",
      "  ✓ Figure 29 analyzed (page 19)\n",
      "  ✓ Figure 29 analyzed (page 19)\n",
      "  ✓ Figure 30 analyzed (page 20)\n",
      "  ✓ Figure 30 analyzed (page 20)\n",
      "  ✓ Figure 31 analyzed (page 22)\n",
      "  ✓ Figure 31 analyzed (page 22)\n",
      "  ✓ Figure 32 analyzed (page 22)\n",
      "  ✓ Figure 32 analyzed (page 22)\n",
      "  ✓ Figure 33 analyzed (page 23)\n",
      "  ✓ Figure 33 analyzed (page 23)\n",
      "  ✓ Figure 34 analyzed (page 23)\n",
      "  ✓ Figure 34 analyzed (page 23)\n",
      "  ✓ Figure 35 analyzed (page 24)\n",
      "  ✓ Figure 35 analyzed (page 24)\n",
      "  ✓ Figure 36 analyzed (page 25)\n",
      "  ✓ Figure 36 analyzed (page 25)\n",
      "  ✓ Figure 37 analyzed (page 29)\n",
      "  ✓ Figure 37 analyzed (page 29)\n",
      "  ✓ Figure 38 analyzed (page 29)\n",
      "  ✓ Figure 38 analyzed (page 29)\n",
      "  ✓ Figure 39 analyzed (page 29)\n",
      "  ✓ Figure 39 analyzed (page 29)\n",
      "  ✓ Figure 40 analyzed (page 29)\n",
      "  ✓ Figure 40 analyzed (page 29)\n",
      "  ✓ Figure 41 analyzed (page 29)\n",
      "  ✓ Figure 41 analyzed (page 29)\n",
      "  ✓ Figure 42 analyzed (page 30)\n",
      "  ✓ Figure 42 analyzed (page 30)\n",
      "  ✓ Figure 43 analyzed (page 30)\n",
      "  ✓ Figure 43 analyzed (page 30)\n",
      "  ✓ Figure 44 analyzed (page 32)\n",
      "  ✓ Figure 44 analyzed (page 32)\n",
      "  ✓ Figure 45 analyzed (page 33)\n",
      "  ✓ Figure 45 analyzed (page 33)\n",
      "  ✓ Figure 46 analyzed (page 36)\n",
      "  ✓ Figure 46 analyzed (page 36)\n",
      "  ✓ Figure 47 analyzed (page 38)\n",
      "  ✓ Figure 47 analyzed (page 38)\n",
      "  ✓ Figure 48 analyzed (page 52)\n",
      "  ✓ Figure 48 analyzed (page 52)\n",
      "  ✓ Figure 49 analyzed (page 52)\n",
      "  ✓ Figure 49 analyzed (page 52)\n",
      "  ✓ Figure 50 analyzed (page 54)\n",
      "  ✓ Figure 50 analyzed (page 54)\n",
      "  ✓ Figure 51 analyzed (page 55)\n",
      "  ✓ Figure 51 analyzed (page 55)\n",
      "  ✓ Figure 52 analyzed (page 56)\n",
      "  ✓ Figure 52 analyzed (page 56)\n",
      "  ✓ Figure 53 analyzed (page 57)\n",
      "  ✓ Figure 53 analyzed (page 57)\n",
      "  ✓ Figure 54 analyzed (page 58)\n",
      "  ✓ Figure 54 analyzed (page 58)\n",
      "  ✓ Figure 55 analyzed (page 58)\n",
      "  ✓ Figure 55 analyzed (page 58)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Extract chapters with Content Understanding\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use Content Understanding to extract chapter information\n",
    "response = content_understanding_client.begin_analyze(DOCUMENT_ANALYZER_ID, file_location=file)\n",
    "chapter_result = content_understanding_client.poll_result(response, timeout_seconds=1000)\n",
    "\n",
    "# Parse chapter information\n",
    "chapters = []\n",
    "document_metadata = {}\n",
    "\n",
    "result_data = chapter_result.get(\"result\", {})\n",
    "contents = result_data.get(\"contents\", [])\n",
    "\n",
    "if contents:\n",
    "    content_fields = contents[0].get(\"fields\", {})\n",
    "    \n",
    "    # Extract document-level metadata\n",
    "    document_metadata = {\n",
    "        \"title\": content_fields.get(\"document_title\", {}).get(\"valueString\", \"\"),\n",
    "        \"summary\": content_fields.get(\"document_summary\", {}).get(\"valueString\", \"\")\n",
    "    }\n",
    "    \n",
    "    # Extract chapters\n",
    "    if \"chapters\" in content_fields and content_fields[\"chapters\"][\"type\"] == \"array\":\n",
    "        chapters_array = content_fields[\"chapters\"][\"valueArray\"]\n",
    "        for chapter_data in chapters_array:\n",
    "            if chapter_data[\"type\"] == \"object\":\n",
    "                chapter_obj = chapter_data[\"valueObject\"]\n",
    "                \n",
    "                # Parse key topics from comma-separated string\n",
    "                key_topics_str = chapter_obj.get(\"key_topics\", {}).get(\"valueString\", \"\")\n",
    "                key_topics = [t.strip() for t in key_topics_str.split(\",\") if t.strip()]\n",
    "                \n",
    "                # Parse page range from \"start-end\" format\n",
    "                page_range_str = chapter_obj.get(\"page_range\", {}).get(\"valueString\", \"0-0\")\n",
    "                try:\n",
    "                    page_start, page_end = map(int, page_range_str.split(\"-\"))\n",
    "                except:\n",
    "                    page_start, page_end = 0, 0\n",
    "                \n",
    "                # Parse level from string to integer\n",
    "                level_str = chapter_obj.get(\"level\", {}).get(\"valueString\", \"1\")\n",
    "                try:\n",
    "                    level = int(level_str)\n",
    "                except:\n",
    "                    level = 1\n",
    "                \n",
    "                chapter = {\n",
    "                    \"title\": chapter_obj.get(\"title\", {}).get(\"valueString\", \"\"),\n",
    "                    \"number\": chapter_obj.get(\"number\", {}).get(\"valueString\", \"\"),\n",
    "                    \"level\": level,\n",
    "                    \"summary\": chapter_obj.get(\"summary\", {}).get(\"valueString\", \"\"),\n",
    "                    \"key_topics\": key_topics,\n",
    "                    \"page_start\": page_start,\n",
    "                    \"page_end\": page_end\n",
    "                }\n",
    "                chapters.append(chapter)\n",
    "\n",
    "print(f\"✓ Extracted {len(chapters)} chapters\")\n",
    "if document_metadata.get('title'):\n",
    "    print(f\"✓ Document: {document_metadata.get('title')}\")\n",
    "for i, chapter in enumerate(chapters, 1):\n",
    "    chapter_num = f\"{chapter['number']}: \" if chapter['number'] else \"\"\n",
    "    print(f\"  {i}. {chapter_num}{chapter['title']} (Pages {chapter['page_start']}-{chapter['page_end']}, Level {chapter['level']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: Extract layout and figures with Document Intelligence\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run Content Understanding on each figure, format figure contents, and insert figure contents into corresponding document locations\n",
    "with open(file, 'rb') as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "        api_version=AZURE_DOCUMENT_INTELLIGENCE_API_VERSION,\n",
    "        credential=credential,\n",
    "        output=str('figures')\n",
    "    )\n",
    "\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        AnalyzeDocumentRequest(bytes_source=pdf_bytes),\n",
    "        output=[str('figures')],\n",
    "        features=['ocrHighResolution'],\n",
    "        output_content_format=\"markdown\"\n",
    "    )\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    \n",
    "    md_content = result.content\n",
    "\n",
    "    print(f\"✓ Extracted document content ({len(md_content)} characters)\")\n",
    "    print(f\"✓ Detected {len(result.figures) if result.figures else 0} figures\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 3: Analyze figures with Content Understanding\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    figure_contents = []\n",
    "    if result.figures:\n",
    "        print(\"Extracting figure contents with Content Understanding image analyzer...\")\n",
    "        for figure_idx, figure in enumerate(result.figures):\n",
    "            for region in figure.bounding_regions:\n",
    "                    # Uncomment the below to print out the bounding regions of each figure\n",
    "                    # print(f\"Figure {figure_idx + 1} body bounding regions: {region}\")\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    bounding_box = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "            page_number = figure.bounding_regions[0]['pageNumber']\n",
    "            cropped_img = crop_image_from_pdf_page(file, page_number - 1, bounding_box)\n",
    "\n",
    "            os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "            figure_filename = f\"figure_{figure_idx + 1}.png\"\n",
    "            # Full path for the file\n",
    "            figure_filepath = os.path.join(\"figures\", figure_filename)\n",
    "\n",
    "            # Save the figure\n",
    "            cropped_img.save(figure_filepath)\n",
    "            bytes_io = io.BytesIO()\n",
    "            cropped_img.save(bytes_io, format='PNG')\n",
    "            cropped_img = bytes_io.getvalue()\n",
    "\n",
    "            # Collect formatted content from the figure\n",
    "            content_understanding_response = content_understanding_client.begin_analyze(ANALYZER_ID, figure_filepath)\n",
    "            content_understanding_result = content_understanding_client.poll_result(content_understanding_response, timeout_seconds=1000)\n",
    "            figure_content = format_content_understanding_result(content_understanding_result)\n",
    "            figure_contents.append(figure_content)\n",
    "            print(f\"  ✓ Figure {figure_idx + 1} analyzed (page {page_number})\")\n",
    "\n",
    "        print(f\"\\n✓ Analyzed {len(figure_contents)} figures\")\n",
    "\n",
    "        # Insert figure content into corresponding location in document\n",
    "        md_content = insert_figure_contents(md_content, figure_contents, [f.spans[0][\"offset\"] for f in result.figures])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 4: Save enhanced document with chapter metadata\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Save enhanced document with both figure descriptions and chapter metadata\n",
    "    result.content = md_content\n",
    "    output = {\n",
    "        \"analyzeResult\": result.as_dict(),\n",
    "        \"chapters\": chapters,\n",
    "        \"document_metadata\": document_metadata\n",
    "    }\n",
    "    output_json = json.dumps(output)\n",
    "    \n",
    "    with open('sample_report.cache', 'w') as f:\n",
    "        f.write(output_json)\n",
    "\n",
    "    print(f\"✓ Saved enhanced document to sample_report.cache\")\n",
    "    print(f\"  - {len(chapters)} chapters with metadata\")\n",
    "    print(f\"  - {len(figure_contents)} figures analyzed\")\n",
    "    print(f\"  - {len(md_content)} characters in markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the first line below to load in a previously cached result.\n",
    "# output_json = open(\"sample_report.cache\").read()\n",
    "document_data = json.loads(output_json)\n",
    "document_content = document_data['analyzeResult']['content']\n",
    "chapters = document_data.get('chapters', [])\n",
    "document_metadata = document_data.get('document_metadata', {})\n",
    "\n",
    "print(f\"Loaded document with {len(chapters)} chapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk text by splitting with Markdown header splitting and recursive character splitting\n",
    "This is a simple starting point. Feel free to give your own chunking strategies a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Configure langchain text splitting settings\n",
    "EMBEDDING_CHUNK_SIZE = 512\n",
    "EMBEDDING_CHUNK_OVERLAP = 20\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\")\n",
    "]\n",
    "\n",
    "# First split text using Markdown headers\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "chunks = text_splitter.split_text(document_content)\n",
    "\n",
    "# Then further split the text using recursive character text splitting\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(separators=[\"<!--\", \"\\n\\n\", \"#\"], chunk_size=EMBEDDING_CHUNK_SIZE, chunk_overlap=EMBEDDING_CHUNK_OVERLAP, is_separator_regex=True)\n",
    "chunks = char_text_splitter.split_documents(chunks)\n",
    "\n",
    "# Helper function to find which chapter a chunk belongs to\n",
    "def find_chapter_for_chunk(chunk_content, chapters):\n",
    "    \"\"\"\n",
    "    Determines which chapter a chunk belongs to based on content analysis.\n",
    "    Returns chapter metadata or None.\n",
    "    \"\"\"\n",
    "    # Check if the chunk contains chapter titles or headers\n",
    "    for chapter in chapters:\n",
    "        if chapter['title'].lower() in chunk_content.lower():\n",
    "            return chapter\n",
    "    return None\n",
    "\n",
    "# Enrich chunks with chapter metadata\n",
    "enriched_chunks = []\n",
    "for chunk in chunks:\n",
    "    # Find which chapter this chunk belongs to\n",
    "    chapter = find_chapter_for_chunk(chunk.page_content, chapters)\n",
    "    \n",
    "    # Add chapter metadata to chunk\n",
    "    if chapter:\n",
    "        chunk.metadata.update({\n",
    "            \"chapter_title\": chapter['title'],\n",
    "            \"chapter_number\": chapter['number'],\n",
    "            \"chapter_level\": chapter['level'],\n",
    "            \"chapter_summary\": chapter['summary'],\n",
    "            \"chapter_topics\": \", \".join(chapter['key_topics']),\n",
    "            \"page_start\": chapter['page_start'],\n",
    "            \"page_end\": chapter['page_end']\n",
    "        })\n",
    "    \n",
    "    # Add document-level metadata\n",
    "    chunk.metadata.update({\n",
    "        \"document_title\": document_metadata.get('title', ''),\n",
    "        \"document_summary\": document_metadata.get('summary', '')\n",
    "    })\n",
    "    \n",
    "    enriched_chunks.append(chunk)\n",
    "\n",
    "print(f\"Number of chunks: {len(enriched_chunks)}\")\n",
    "print(f\"Chunks with chapter metadata: {sum(1 for c in enriched_chunks if 'chapter_title' in c.metadata)}\")\n",
    "\n",
    "# Show sample chunk with metadata\n",
    "if enriched_chunks:\n",
    "    print(f\"\\nSample chunk metadata:\")\n",
    "    if 'chapter_title' in enriched_chunks[0].metadata:\n",
    "        print(f\"  Chapter: {enriched_chunks[0].metadata.get('chapter_title')}\")\n",
    "        print(f\"  Topics: {enriched_chunks[0].metadata.get('chapter_topics')}\")\n",
    "    print(f\"  Document: {enriched_chunks[0].metadata.get('document_title', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query vector index to retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "                                        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "                                        azure_ad_token_provider=token_provider,\n",
    "                                        api_version=AZURE_OPENAI_EMBEDDING_API_VERSION)\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    azure_search_key=None,\n",
    "    index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "    embedding_function=aoai_embeddings.embed_query\n",
    ")\n",
    "\n",
    "# This is a one-time operation to add the documents to the vector store. Comment out this line if you are re-running this cell with the same index.\n",
    "print(f\"Indexing {len(enriched_chunks)} enriched chunks with chapter metadata...\")\n",
    "vector_store.add_documents(documents=enriched_chunks)\n",
    "print(\"✓ Indexing complete\")\n",
    "\n",
    "# Set up the retriever that will be used to query the index for similar documents\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "print(\"✓ Vector store ready for chapter-aware search!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant documents\n",
    "query = \"What was the crude oil production in 2019?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "\n",
    "# Print retrieved documents with chapter information\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  ID: {doc.metadata.get('id', 'N/A')}\")\n",
    "    if 'chapter_title' in doc.metadata:\n",
    "        print(f\"  Chapter: {doc.metadata.get('chapter_title')}\")\n",
    "        print(f\"  Chapter Topics: {doc.metadata.get('chapter_topics')}\")\n",
    "        print(f\"  Pages: {doc.metadata.get('page_start')}-{doc.metadata.get('page_end')}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answer to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt template for chat model\n",
    "prompt = \"\"\"\n",
    "You are an expert in document analysis. You are proficient in reading and analyzing technical reports. You are good at numerical reasoning and have a good understanding of financial concepts. You are given a question which you need to answer based on the references provided. To answer this question, you may first read the question carefully to know what information is required or helpful to answer the question. Then, you may read the references to find the relevant information.\n",
    "\n",
    "If you find enough information to answer the question, you can first write down your thinking process and then provide a concise answer at the end.\n",
    "If you find that there is not enough information to answer the question, you can state that there is insufficient information.\n",
    "If you are not able or sure how to answer the question, say that you are not able to answer the question.\n",
    "Do not provide any information that is not present in the references.\n",
    "References are in markdown format, you may follow the markdown syntax to better understand the references.\n",
    "\n",
    "---\n",
    "References:\n",
    "{context}\n",
    "---\n",
    "\n",
    "Now, here is the question:\n",
    "---\n",
    "Question:\n",
    "{question}\n",
    "---\n",
    "Thinking Process::: \n",
    "Answer::: \n",
    "\"\"\"\n",
    "\n",
    "# Helper function to generate the formatted context from each retrieved document\n",
    "def generate_context(chunks):\n",
    "    context = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        s = (f'Source {i} Metadata: {chunk.metadata}\\n'\n",
    "                f'Source {i} Content: {chunk.page_content}')\n",
    "        context.append(s)\n",
    "    context = '\\n---\\n'.join(context)\n",
    "    return context\n",
    "\n",
    "# Remove redundant chunks\n",
    "appeared = set()\n",
    "unique_chunks = []\n",
    "for chunk in retrieved_docs:\n",
    "    chunk_id = chunk.metadata['id']\n",
    "    if chunk_id not in appeared:\n",
    "        appeared.add(chunk_id)\n",
    "        unique_chunks.append(chunk)\n",
    "context = generate_context(unique_chunks)\n",
    "\n",
    "# Format the prompt with the provided query and formatted context\n",
    "prompt = prompt.format(question=query,\n",
    "                       context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "chat_llm = AzureChatOpenAI(model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                            azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "                            azure_ad_token_provider=token_provider,\n",
    "                            api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "                            temperature=0.7)\n",
    "\n",
    "# Print the LLM's answer to the query with the retrieved documents as additional context\n",
    "answer = chat_llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
